---
layout: post
title: RL-based Market Microstructure Strategy
---

My latest personal project involved diving into the world of market microstructure and attempting to apply Reinforcement Learning to optimize trade execution. Honestly, it was a steep learning curve, dealing with tick data and the sheer complexity of high-frequency trading environments, even in simulation.

The goal was pretty specific: develop an RL agent that could learn to execute a larger order by breaking it down into smaller trades over time, aiming to minimize market impact and improve the final average fill price. This is classic optimal execution, but I wanted to see if an RL approach, specifically trained on historical tick data, could dynamically adapt better than traditional algorithms.

I chose PyTorch because I wanted to get more hands-on experience with it beyond simple tutorials. Plus, its dynamic graph seemed potentially useful for building complex agent architectures, though that didn't end up being as critical as I initially thought. The main appeal was its flexibility for building custom network layers and loss functions.

Setting up the data was the first major hurdle. Historical tick data is massive. We're talking timestamps down to nanoseconds, price, volume, and order book updates. I managed to get a sample dataset for a specific stock over a few days. Just loading and cleaning it was tedious. Initially, I tried loading chunks with `pandas.read_csv`, but even that felt clunky. Eventually, I wrote a custom data loader in Python that would stream data from compressed files, processing it on the fly to create the necessary features. This involved wrestling with low-level file I/O and figuring out efficient ways to compute moving averages or order book pressure indicators without loading everything into memory at once. There were definitely frustrating moments trying to get the indexing right across different data streams.

The core of the project was defining the Reinforcement Learning problem. This was probably the most challenging conceptual part.
*   **State Space:** What information does the agent need to make a decision *right now*? Tick data is overwhelming. I settled on a state vector including current time remaining until the order deadline, the volume remaining to be traded, recent price volatility (SMA of price changes), recent volume trends, and some aggregated order book features (e.g., total volume within X ticks of the best bid/ask). Representing the order book depth effectively without the state space exploding took a few iterations. My initial idea was just raw levels, but that was too noisy and inconsistent; aggregating by price buckets worked much better.
*   **Action Space:** This was simpler. At each decision step (say, every 100 milliseconds or after a certain volume threshold traded), the agent needed to decide how much volume to trade and at what price (market or limit). I simplified the action space to a discrete set: do nothing, submit a market order of size X, submit a limit order of size X at price Y. Defining X and Y dynamically based on state was tricky; I ended up making them relative to the remaining volume and current best bid/ask.
*   **Reward Function:** This is where the agent learns. The primary goal is to execute the full volume by the deadline with the best average price. I defined the reward as a combination of the volume filled in the current step and a penalty proportional to the slippage (difference between execution price and the mid-price at the time of action). There was also a small penalty for time running out before full execution. Getting the balance right between filling quickly and getting a good price was hard. My first attempts had the agent either doing nothing (avoiding penalties) or just market-ordering everything instantly (getting poor prices). Tweak, test, tweak again. I remember spending hours staring at training logs, trying to figure out why the agent wasn't learning a sensible strategy. Reading some papers and a couple of blog posts discussing reward shaping in financial RL helped clarify the need for explicit slippage penalties.

For the agent itself, I used a standard Deep Q-Network (DQN) architecture, though I later experimented with Advantage Actor-Critic (A2C) as well. The DQN implementation in PyTorch involved:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class QNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(QNetwork, self).__init__()
        # Started simple
        self.fc1 = nn.Linear(state_size, 128)
        self.fc2 = nn.Linear(128, 128)
        # Output Q-values for each discrete action
        self.fc3 = nn.Linear(128, action_size)

    def forward(self, state):
        # Using ReLU activations
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

# ... later in the training loop ...
# Q_expected = self.local_qnetwork(state_batch).gather(1, action_batch)
# Q_targets = self.target_qnetwork(next_state_batch).detach().max(1)[0].unsqueeze(1)
# loss = F.mse_loss(Q_expected, Q_targets)
# optimizer.zero_grad()
# loss.backward()
# optimizer.step()
```
This basic structure worked, but I found that adding batch normalization and dropout helped stabilize training, especially with the noisy financial data. I also considered using LSTMs or GRUs to process the sequential nature of tick data, but decided against it initially due to increased complexity and training time. Sticking with feed-forward layers simplified the model and allowed me to get a baseline working faster.

Training was... challenging. Replay buffers for millions of ticks required careful memory management. Deciding on hyperparameters like learning rate, discount factor (`gamma`), and exploration rate decay (`epsilon`) felt like guesswork initially. I ran many training runs, adjusting these values based on how the agent's performance improved (or didn't) over episodes. An episode was defined as trying to execute one larger order from start to finish within the time limit, using data from a specific time window. I trained on data from one set of days and validated on another.

Debugging involved a lot of printing intermediate values â€“ the agent's chosen action probabilities, the calculated reward, the state vector. One time, I realized the state normalization was off, leading to huge gradients and unstable training. Another time, the `target_qnetwork` wasn't updating correctly, which meant the agent was chasing a stale target. Caught that after painstakingly stepping through the training loop logic based on the DQN paper.

Evaluation involved simulating the trained agent's actions on unseen historical data. I built a simple event-driven simulator that processed the tick data sequentially, fed the state to the agent at decision points, recorded its actions, and updated the simulated order book and agent's state based on executed trades. Comparing the average fill price and the percentage of volume executed within the deadline against a simple baseline strategy (e.g., submitting fixed-size limit orders spread out over time) showed that the RL agent could achieve a better average fill price for the same amount of volume executed, especially in volatile periods. The "improved fill rates" mentioned in the project summary were specifically these better average execution prices compared to the baseline. It wasn't a massive improvement, maybe a few basis points on average, but considering the noise and difficulty, I considered it a success for a first attempt.

Overall, this project was a solid exercise in applying RL to a real-world, noisy problem. The biggest takeaways were the importance of careful problem formulation (state, action, reward), the challenges of handling large-scale financial data, and the iterative nature of hyperparameter tuning and debugging in deep learning projects. It definitely wasn't a smooth ride, but getting the agent to actually learn a non-trivial execution strategy felt like a genuine breakthrough after many hours spent debugging. Next steps would probably involve trying the LSTM approach or exploring more advanced RL algorithms like PPO.