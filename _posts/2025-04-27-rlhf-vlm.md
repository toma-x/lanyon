---
layout: post
title: RLHF for Vision-Text Alignment
---

Okay, so I finished up that RLHF project I mentioned working on. The goal was pretty specific: take a VLLM and get it to generate *better* captions for complex images, defined by human preference. Basically, aligning the VLLM's output with what people actually want to see in a detailed caption, especially for tricky scenes.

My plan centered around Reinforcement Learning from Human Feedback (RLHF). This involved training a reward model to score captions based on human preference data, and then using that model to guide the fine-tuning of the VLLM using an RL algorithm.

The initial phase, getting the human feedback and training the reward model, took up a good chunk of time â€“ maybe about two weeks total, working evenings and weekends. The hardest part wasn't the coding itself, but defining the annotation guidelines. For complex images, what makes a caption "good"? Is it detail? Specificity? Avoiding hallucination? We ended up focusing on relevance to key objects/actions and the level of detail provided. Gathering and cleaning this data was... tedious. Pandas became my best friend and occasional enemy.

For the reward model, I went with a fairly standard Transformer-based architecture, specifically adapting a pre-trained text classifier. I briefly considered something simpler like an LSTM, but given the complexity of caption nuances, a Transformer felt necessary to capture longer-range dependencies and context. Training wasn't completely smooth. I hit issues with the loss plateauing prematurely. Turns out, my initial learning rate schedule was too aggressive. After digging through PyTorch Lightning docs and a few Stack Overflow threads (specifically remember one about `ReduceLROnPlateau` configuration), I adjusted it, and the model started converging properly. My early versions of the training loop had some silly bugs with device placement too; kept forgetting to move tensors to the GPU after some operations, leading to `RuntimeError: Expected all tensors to be on the same device...` This wasted a few hours debugging before I added explicit `.to(device)` calls everywhere.

Here's a snippet from the reward model training setup:

```python
# Initial naive setup - device placement issues here!
# class RewardModel(nn.Module): ...
# model = RewardModel().to(device)
# optimizer = AdamW(model.parameters(), lr=5e-5) # Too high maybe?

# Corrected setup snippet
class RewardModel(pl.LightningModule): # Using PyTorch Lightning helped manage devices
    def __init__(self, pretrained_model_name="bert-base-uncased"): # Simplified for example
        super().__init__()
        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels=1) # Regression for score
        # Maybe try LoRA here later? Not for this version.
        self.save_hyperparameters()

    def forward(self, input_ids, attention_mask):
        return self.model(input_ids=input_ids, attention_mask=attention_mask).logits # Output is a score

    def training_step(self, batch, batch_idx):
        # batch = {key: val.to(self.device) for key, val in batch.items()} # PL handles this!
        inputs = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}
        scores = self(batch['input_ids'], batch['attention_mask']).squeeze(-1)
        # Assuming batch['labels'] contains pairwise preferences (chosen_score > rejected_score)
        # Simpler MSE loss initially, but switched to pairwise ranking loss later based on papers
        # loss = F.mse_loss(scores, batch['target_scores']) # Initial mistake: trying to predict absolute scores
        # Corrected: Pairwise ranking loss
        chosen_scores = self(batch['chosen_input_ids'], batch['chosen_attention_mask']).squeeze(-1)
        rejected_scores = self(batch['rejected_input_ids'], batch['rejected_attention_mask']).squeeze(-1)
        loss = -F.logsigmoid(chosen_scores - rejected_scores).mean() # Standard ranking loss
        self.log('train_loss', loss)
        return loss

    # ... validation_step, configure_optimizers ...

# Later in training script:
# model = RewardModel(...)
# trainer = pl.Trainer(max_epochs=3, accelerator='gpu') # Using GPU!
# trainer.fit(model, data_module) # Use a DataModule for data loading
```
My initial thought was to train the reward model to predict an absolute "goodness" score (like 1-5), but after reading some papers and realizing the human feedback was inherently pairwise ("caption A is better than caption B"), I switched to a pairwise ranking loss. That was a minor "aha!" moment that improved the reward signal quality significantly.

The second phase, RL fine-tuning the VLLM, was definitely the most challenging and time-consuming part, taking nearly three weeks. I used PPO (Proximal Policy Optimization) primarily because I had some familiarity with it from previous projects and open-source implementations for language models (like Hugging Face's TRL library) are relatively mature. I considered simpler methods like REINFORCE, but instability worried me.

Integrating the reward model output into the VLLM training loop was tricky. The VLLM was large, and fine-tuning the full model was computationally prohibitive on my setup (a single 3090). I opted for LoRA (Low-Rank Adaptation) to keep memory usage manageable and training faster. This meant I was only training small adapter layers added to the pre-trained VLLM. Setting up the PPO loop with the LoRA model and the separate reward model required careful state management.

I ran into persistent issues with training divergence early on. The KL divergence penalty, a key part of PPO to keep the fine-tuned policy close to the original, needed careful tuning. My initial `beta` parameter for the KL penalty was too low, letting the policy drift too far and generate nonsensical text to game the reward model. Increasing `beta` helped stabilize training, but too high a value prevented the model from learning to increase the reward. Finding that balance required a lot of experimentation and watching metrics like the average reward per sample and the KL divergence metric provided by the PPO trainer.

A specific issue I battled for a couple of days was a mismatch between the tokenizers used by the VLLM and the reward model. Even though they were both based on similar models, subtle differences in special tokens or padding could cause the reward model to output garbage scores. I had to ensure they used the *exact* same tokenizer instance and padding settings when processing the generated captions during the RL step. Debugging involved printing token IDs and attention masks at various points to see where they diverged. It felt pretty basic, but it was a real roadblock.

Here's a simplified look at the RL loop idea:

```python
# Pseudocode / concept for the RL loop
# Assuming policy_model is the VLLM with LoRA adapters, reward_model is trained
# Using concepts similar to Hugging Face TRL library

# Config
# ppo_config = PPOConfig(gamma=0.99, beta=0.1, ...) # beta tuning was key!
# ppo_trainer = PPOTrainer(model=policy_model, ref_model=policy_model_ref, # Use a frozen copy for KL
#                          config=ppo_config, dataset=rl_dataset,
#                          tokenizer=tokenizer) # Use THE SAME tokenizer!

# RL Training Loop (simplified)
# for epoch in range(num_epochs):
#    for batch in data_loader:
#        # Get prompt images/texts
#        prompts = batch['prompts'] # Image tensors or image + text context

#        # Generate captions from the current policy (VLLM with LoRA)
#        # This involves the VLLM's generation logic
#        # Need to handle beam search, sampling, etc. - complexity hidden here
#        generated_captions, generation_info = ppo_trainer.generate(prompts, ...) # TRL handles generation

#        # Tokenize generated captions for the reward model
#        # THIS IS WHERE TOKENIZER MISMATCH CAUSED PAIN
#        reward_inputs = reward_tokenizer(generated_captions, ...)

#        # Get reward from the reward model
#        with torch.no_grad(): # Don't train the reward model here!
#             # Ensure reward_inputs are on the correct device
#             reward_inputs = {k: v.to(reward_model.device) for k, v in reward_inputs.items()}
#             rewards = reward_model(**reward_inputs).squeeze(-1)

#        # Perform PPO step
#        # This updates the LoRA adapters of the policy_model based on rewards and KL penalty
#        # Need to provide the original generation log probs for KL calculation (generation_info had this)
#        stats = ppo_trainer.step(prompts, generated_captions, rewards)

#        # Log metrics (reward, KL, policy/value loss)
#        # print(f"Epoch {epoch}, Batch {batch_idx}: Avg Reward = {stats['mean/reward']:.2f}")
```

The training was slow, even with LoRA. Each epoch took hours. I ran it for about 5-6 epochs, which felt like a reasonable trade-off between training time and potential improvement given my compute resources. If I had access to more GPUs, I'd probably train longer and maybe experiment with larger batch sizes for the RL step.

Finally, evaluation. This took another week to do properly. The 18% improvement figure came from a blind human evaluation comparing captions from the original VLLM and the RLHF fine-tuned version on a held-out set of complex images. We used the same kind of pairwise comparison feedback as for the reward model training data, but with a different set of evaluators. The metric wasn't a simple average score increase, but rather the win rate of the RLHF model's captions against the baseline's captions in head-to-head comparisons. We saw improvements in both relevance (captions mentioned objects that evaluators considered important) and detail (captions included more specific attributes or actions). There were still cases where the RLHF model generated awkward phrasing or missed obvious details, but overall, the trend was positive and statistically significant based on our evaluation set size.

The stack was primarily Python, PyTorch, Hugging Face Transformers and PEFT (for LoRA), TRL (for PPO), and PyTorch Lightning for training management. Data handling used Pandas and standard libraries.

Overall, it was a complex project that stretched my understanding of both large models and reinforcement learning. The biggest takeaway was how sensitive these RLHF setups are to hyperparameters and subtle data/tokenization issues. It definitely felt like navigating a minefield at times, but seeing the quantitative improvement in caption quality made the debugging hours worth it. Next steps might involve trying different RL algorithms or exploring methods to make the reward model more robust.