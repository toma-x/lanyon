---
layout: post
title: Low-Latency Order Book Simulator
---

Building a Low-Latency Order Book Simulator in C++20

After spending time wrestling with Reinforcement Learning agents for market microstructure problems, I quickly realized a major bottleneck: the simulation environment itself. Training an agent requires simulating millions, if not billions, of interactions with a market. Existing simulators I found were either too slow, too complex to modify, or didn't offer the granular control needed for low-latency strategies. This led me down a path to build my own: an ultra-low-latency order book simulation engine from the ground up, written in C++20. The goal was ambitious – process market events at nanosecond-level speeds to enable rapid iteration on trading logic.

Honestly, jumping into writing a performance-critical system in C++ after spending months in Python and PyTorch was a bit jarring. Memory management, pointers, and the sheer complexity of templates felt like relearning a language. But the performance requirements dictated the choice; Python just wouldn't cut it for the kind of throughput needed. C++20 offered some nice features, particularly with concepts and ranges, though I ended up relying more on core language features and standard library containers, focusing heavily on how they behave in terms of performance.

The core problem was managing the order book data structure itself. An order book consists of two sides: bids (buy orders) and asks (sell orders), each sorted by price, with orders at the same price sorted by time (price-time priority). Events like new orders, cancellations, and executions constantly modify this structure. Speed here is everything.

My initial thought process, coming from higher-level languages, was to use standard containers. A `std::map<double, std::vector<Order>>` seemed intuitive enough: map price levels to a list of orders at that price. Adding an order would involve finding the price level (or inserting a new one) and adding the order to the vector. Matching involved finding the best bid/ask and iterating through the vector.

```cpp
// Initial, naive thought (simplified)
struct Order {
    long long order_id;
    double price;
    long long quantity;
    long long timestamp;
    // ... other fields
};

// Bids sorted descending by price, Asks ascending
std::map<double, std::vector<Order>> bids;
std::map<double, std::vector<Order>> asks; // Need custom comparator for ascending price

// Adding an order (simplified)
// auto& orders_at_price = bids[order.price]; // This inserts if not found
// orders_at_price.push_back(order); // Simple add, ignores time priority for now
```

This approach worked logically but was *slow*. Profiling (using `perf` on Linux) quickly showed that `std::map` operations (inserting, finding) and `std::vector` manipulations (especially inserting into the middle to maintain time priority, which I quickly realized was necessary) were massive bottlenecks. `std::map`'s node-based structure means poor cache locality, and its O(log N) operations, while theoretically okay, add up quickly under high event rates. `std::vector` insertions/deletions in the middle are O(N), which is a non-starter.

I spent a frustrating couple of days trying to optimize *around* this structure – maybe using iterators more effectively, or trying `std::unordered_map` (which failed immediately because I need *sorted* prices). I consulted StackOverflow and articles discussing high-performance order books. The consensus seemed to be that standard maps were too slow for high-frequency trading (HFT) systems, and specialized structures or careful use of arrays/vectors with indices were necessary.

The breakthrough came from realizing that prices, while potentially numerous, have a limited dynamic range relative to the minimum price tick. Instead of a map keying off `double` (which has its own issues with floating-point comparisons anyway), I could key off an integer representation of the price (e.g., price in cents or tenths of cents). More importantly, for ultra-low latency, avoiding frequent dynamic memory allocations and maximizing cache hits was paramount.

The revised approach involved using arrays or vectors where possible and managing memory more explicitly. I opted for a structure that combined quick price level lookup with efficient time-priority within levels. This involved using a large, pre-allocated array or vector indexed by the integer price representation, where each element pointed to a structure managing orders at that specific price level. For time priority within a price level, a simple doubly-linked list seemed appropriate, as insertions and deletions are O(1) (once the node is found).

```cpp
// Optimized approach (conceptual)
const int MAX_PRICE_LEVELS = 100000; // Example: Max price range / tick size
struct OrderNode {
    Order order; // Contains ID, qty, timestamp, etc.
    OrderNode* prev;
    OrderNode* next;
    // Maybe pool index instead of pointers later for cache?
};

struct PriceLevel {
    OrderNode* head; // Head of the time-priority linked list
    OrderNode* tail;
    // Stats: total volume at this level, etc.
};

// Use vector/array indexed by integer price level
std::vector<PriceLevel> bid_levels; // Need to map integer price to index
std::vector<PriceLevel> ask_levels; // ... and handle the price range mapping

// Example: Adding order to a price level (simplified)
// int price_idx = map_price_to_index(order.price);
// if (price_idx is valid and level exists) {
//     OrderNode* new_node = get_node_from_pool(); // Avoids 'new'
//     new_node->order = order;
//     // Add new_node to the end of the linked list at bid_levels[price_idx]
//     // Need to manage head/tail pointers carefully
// } else {
//     // Handle new price level or invalid price
// }

// Matching logic iterates from best bid/ask indices,
// then iterates through the linked list at that level.
```

This design, while significantly more complex to implement and manage memory for, drastically reduced the time spent in container operations. Pre-allocating nodes for the linked lists using a simple object pool pattern (getting a free node from a list instead of calling `new`) eliminated allocation overhead in the critical path. Mapping prices to array indices required careful handling of the price range and offset, which was a minor headache to get right initially. I remember a bug where a price mapping was off by one tick, causing orders to land on the wrong price level and breaking matching. Debugging involved stepping through the price mapping logic for specific order examples.

Implementing the matching engine logic – how market orders cross the book, how limit orders interact, handling partial fills and remaining quantities – was intricate. Each event type (limit order, market order, cancel, replace) had its own processing path, requiring updates to price levels, order quantities, and the overall book structure. The logic for removing an order node from the doubly-linked list and returning it to the object pool also needed to be robust to avoid memory leaks or double-free errors.

After implementing the core event processing with the optimized data structures and using an object pool, the performance jump was dramatic. Benchmarking involved feeding a large stream of historical market data events through the simulator and measuring the total time taken and the number of events processed per second. My initial `std::map` based attempt barely broke 100,000 events per second on my machine. With the linked-list-at-price-level and object pool approach, I consistently measured over 1 million events per second – roughly a 10x improvement in throughput, hitting the nanosecond processing goal per simple event like adding a non-crossing limit order. Market order executions, involving multiple order traversals and modifications, took longer but were still within reasonable bounds.

This project reinforced the critical importance of data structure choice and memory management for high-performance computing, something often abstracted away in higher-level languages. Getting the order book structure right, avoiding dynamic allocations where possible, and being mindful of CPU cache behavior were key. It wasn't easy; there were plenty of late nights staring at GDB, trying to figure out why a pointer was null or why the book state was inconsistent after a series of complex trades. But seeing the simulator chew through millions of events in seconds, where before it took minutes, was incredibly rewarding and validated the deep dive into C++ specifics. Next, I'd like to explore adding features like different order types (IOC, FOK) and potentially multi-threading the simulation if the RL training process can be structured that way.