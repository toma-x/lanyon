---
layout: post
title: Vision-Language Generative Model
---

ok so. this VLLM project. wow. where do I even start? maybe like 6 months ago? i dunno, felt like forever. the idea was simple enough right? combine images and text. like, show it a pic and ask a question about it, or make it describe the damn thing. everyone's talking about these massive models but my pathetic laptop (bless its noisy heart) was never gonna handle that. so, "small-scale" VLLM it was.

pytorch, obviously. that's what i know, mostly. tried tensorflow once, wasn't feeling it. setting up the environment was... a process. conda environments are supposed to be easy, right? lies. spent a solid evening just wrestling with CUDA versions and torch compatibility. finally got it working but i'm pretty sure my setup is held together by duct tape and sheer willpower. don't look too closely.

first hurdle: data. "custom curated dataset". sounds so official on the resume. reality? spending hours on google images and some open datasets, downloading stuff, trying to pair images with relevant captions or make up simple Q&A pairs. it was soul-crushing manual labor. like, seriously. hundreds of image-text pairs. i probably introduced so much bias just based on what i could find and what didn't make me want to pull my hair out. there's a reason data scientists get paid, people. this part sucked. i wrote a janky python script to organize files but it was mostly just me in front of a screen going "ok, this dog pic... what's a good question? 'what color is the dog?' yeah, riveting."

then the actual model. ok, VLLM. vision + language. obvious parts: a vision encoder to understand the image, a language model to handle the text.
vision encoder: initial thought, just use like, a pre-trained ResNet, right? easy peasy. grab `resnet50` from `torchvision.models`. slap that on the front. cool. processes the image, gives me a vector. great. but then how do i *give* that to the language model? the language model expects text tokens. this was my first big "wait, what?" moment.
language model: grabbed a small GPT-2 from Hugging Face transformers. seemed like a standard choice, plenty of examples. thought process was, ok, i'll feed it the text (like the start of a caption or the question) and somehow the image info too, and it should generate the rest.

fusion techniques. *this* is where i lived in hell for like, two weeks straight. how do you combine a vision vector (like, a bunch of numbers) with text embeddings (also numbers, but in a sequence)?
first dumb idea: just... concatenate them? like, flatten the image feature vector and stick it at the beginning of the text token embeddings sequence?
```python
# somewhere in the forward pass...
vision_features = self.vision_encoder(image) # shape e.g. (batch_size, 2048)
text_embeddings = self.text_embedding_layer(text_tokens) # shape (batch_size, seq_len, embed_dim)

# uhh... flatten vision?
vision_flat = vision_features.unsqueeze(1).expand(-1, text_embeddings.size(1), -1) # this is wrong lol

# or maybe just stick it at the beginning?
# need to match dimensions... maybe a linear layer?
vision_proj = self.vision_projection(vision_features).unsqueeze(1) # shape (batch_size, 1, embed_dim)
combined_embeddings = torch.cat([vision_proj, text_embeddings], dim=1) # shape (batch_size, seq_len+1, embed_dim)

# feed combined_embeddings to the language model
```
yeah, that `torch.cat` thing felt slightly less stupid. like, projecting the vision features into the same embedding space as the text and treating it as an extra 'visual token' at the start of the sequence. this kinda worked? for image captioning, it would sometimes spit out vaguely related words. but for QA, it was just garbage. Asking "what color is the sky?" with a pic of a blue sky might get "the... dog... is... running...". total nonsense.

spent ages fiddling with that fusion part. read papers, skimmed blog posts (the really dense ones), basically just hitting my head against the keyboard. tried adding cross-attention layers where the language model tokens could 'look at' the vision features. that was even *more* complicated to implement. had to figure out masking, attention weights... my head was spinning. i copied code snippets from github repos and tried to adapt them. probably broke more than i fixed initially. got so many `shape mismatch` errors, i swear pytorch was personally attacking me. i'd print out the `.shape` of like, every single tensor at every step. took forever to find one little dimension that was off by like, 1 or something stupid.

training itself was... slow. on my laptop's pathetic GPU (a GTX 1050 Ti, don't laugh). batch sizes had to be tiny, like 4 or 8. epochs took hours. watching the loss curve slowly (or sometimes, rapidly and worryingly) go down. had issues with overfitting *so* fast because the dataset was small and probably not great. introduced dropout, played with learning rates (Adam optimizer, standard stuff), gradient clipping... all the usual tricks. still felt like playing whack-a-mole. one thing gets better, another gets worse.

debugging `nan` loss values became a nightly ritual. ok, 2 AM, loss is NaN again. where did it go wrong *this* time? print all the gradients! check for division by zero! stare into the void! turns out one time it was just a stupid mistake in my data loading, dividing by 255 *after* normalizing, effectively making everything tiny and zeroing out. facepalm.

alternative vision encoders: i did try swapping ResNet for a Vision Transformer (ViT) for a bit. it was bigger, slower to load, and didn't seem to magically fix my fusion problems, so i went back to ResNet because it was more familiar and faster to iterate with on my limited hardware. constraints, man. you gotta pick your battles.

the 'multimodal QA' part? yeah, that's still kinda flaky. image captioning works... sometimes. it's not perfect, definitely not state-of-the-art, but it produces *something* vaguely coherent for simple images. QA is a coin toss. it feels like the model struggles to really ground the answer in the image content vs just using the text prompt. maybe the fusion isn't strong enough. maybe the dataset is too simple. maybe i'm just not smart enough lol.

overall? frustrating as hell, learned a ton about pytorch internals, tensors, dimensions, and how hard it is to just combine two different types of data meaningfully. my blog post about it will probably be rambling and full of technical jargon only i understand, but hey, i built *something*. my laptop fan hasn't stopped screaming since. worth it? probably. ask me again after a full night's sleep.